{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import glob\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the IGN Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 score_phrase                                              title  \\\n",
      "0           0      Amazing                            LittleBigPlanet PS Vita   \n",
      "1           1      Amazing  LittleBigPlanet PS Vita -- Marvel Super Hero E...   \n",
      "2           2        Great                               Splice: Tree of Life   \n",
      "3           3        Great                                             NHL 13   \n",
      "4           4        Great                                             NHL 13   \n",
      "\n",
      "                                                 url          platform  score  \\\n",
      "0             /games/littlebigplanet-vita/vita-98907  PlayStation Vita    9.0   \n",
      "1  /games/littlebigplanet-ps-vita-marvel-super-he...  PlayStation Vita    9.0   \n",
      "2                          /games/splice/ipad-141070              iPad    8.5   \n",
      "3                      /games/nhl-13/xbox-360-128182          Xbox 360    8.5   \n",
      "4                           /games/nhl-13/ps3-128181     PlayStation 3    8.5   \n",
      "\n",
      "        genre editors_choice  release_year  release_month  release_day  \n",
      "0  Platformer              Y          2012              9           12  \n",
      "1  Platformer              Y          2012              9           12  \n",
      "2      Puzzle              N          2012              9           12  \n",
      "3      Sports              N          2012              9           11  \n",
      "4      Sports              N          2012              9           11  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ign.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert scores in a suitable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique scores:\n",
      " ['Amazing', 'Great', 'Good', 'Awful', 'Okay', 'Mediocre', 'Bad', 'Painful', 'Unbearable', 'Disaster', 'Masterpiece']\n",
      "Mapping from scores to integers:\n",
      " {'Great': 5, 'Disaster': 3, 'Bad': 2, 'Good': 4, 'Painful': 9, 'Unbearable': 10, 'Awful': 1, 'Amazing': 0, 'Okay': 8, 'Mediocre': 7, 'Masterpiece': 6}\n"
     ]
    }
   ],
   "source": [
    "# Our labels will be generated starting from data of column 'score_phrase'\n",
    "labels = list(df['score_phrase'].unique())\n",
    "print(\"Unique scores:\\n\", labels)\n",
    "# To generate a baseline for the classification we convert them to positive and\n",
    "# negative\n",
    "positive = ['Masterpiece', 'Amazing', 'Great', 'Good', 'Okay']\n",
    "negative = ['Awful', 'Mediocre', 'Bad', 'Painful', 'Unbearable', 'Disaster']\n",
    "\n",
    "def convert_to_binary(val):\n",
    "    if val in positive:\n",
    "        return 1#'positive'\n",
    "    elif val in negative:\n",
    "        return 0#'negative'\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "df['binary'] = df['score_phrase'].apply(convert_to_binary)\n",
    "#print(df['binary'].unique())\n",
    "\n",
    "\n",
    "# Create a mapping to integers for the labels, to be used later with a more complicated RNN\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "map_labels = { l : i for l, i in zip(labels, le.transform(labels)) }\n",
    "print(\"Mapping from scores to integers:\\n\", map_labels)\n",
    "df['multiple'] = df['score_phrase'].map(map_labels)\n",
    "#print(df['multiple'][:5])\n",
    "#print(df['score_phrase'][:5])\n",
    "#print(le.transform(['Amazing', 'Great']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'score_phrase', 'title', 'url', 'platform', 'score',\n",
      "       'genre', 'editors_choice', 'release_year', 'release_month',\n",
      "       'release_day', 'binary', 'multiple'],\n",
      "      dtype='object')\n",
      "['Y' 'N']\n",
      "1    3517\n",
      "Name: binary, dtype: int64\n",
      "1    10801\n",
      "0     4307\n",
      "Name: binary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "# We don't have a full review but we can combine several columns to get one.\n",
    "# We assume url and release date are not relevant for the final score\n",
    "print(df['editors_choice'].unique())\n",
    "# 'editors_choice' is composed by Y and N only\n",
    "print(df.loc[df['editors_choice'] == 'Y', 'binary'].value_counts())\n",
    "print(df.loc[df['editors_choice'] == 'N', 'binary'].value_counts())\n",
    "# We can see that such feature is quite important since whenever the game\n",
    "# is an editor's choice it gets always a positive review. Since we want to\n",
    "# perform sentiment analysis on reviwews it is better to convert 'Y' in something\n",
    "# more clear like 'editors_choice'\n",
    "df['editors_choice'] = df['editors_choice'].map({'Y' : 'editors_choice', 'N': ''})\n",
    "\n",
    "to_merge = ['title', 'platform', 'genre', 'editors_choice']\n",
    "\n",
    "def insert_space(val):\n",
    "    if pd.isnull(val):\n",
    "        return ' '\n",
    "    else:\n",
    "        return val + ' '\n",
    "\n",
    "df['predictors'] = df['title'].apply(insert_space)\n",
    "for col in to_merge[1:]:\n",
    "\n",
    "    df['predictors'] += df[col].apply(insert_space)\n",
    "\n",
    "#print(df[['predictors', 'title', 'platform', 'genre', 'editors_choice']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12037    Tomb Raider Underworld PlayStation 3 Action  \n",
      "616           Star Fox 64 Nintendo 64 Flight, Action  \n",
      "Name: predictors, dtype: object\n",
      "  (0, 208)\t1\n",
      "  (0, 4421)\t1\n",
      "  (0, 6141)\t1\n",
      "  (0, 4650)\t1\n",
      "  (0, 5911)\t1\n",
      "  (1, 2294)\t1\n",
      "  (1, 4037)\t1\n",
      "  (1, 154)\t2\n",
      "  (1, 2349)\t1\n",
      "  (1, 5507)\t1\n",
      "  (1, 208)\t1\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(12478, 6625)\n",
      "(12478, 6625)\n",
      "  (0, 208)\t1\n",
      "  (0, 4421)\t1\n",
      "  (0, 6141)\t1\n",
      "  (0, 4650)\t1\n",
      "  (0, 5911)\t1\n",
      "  (1, 2294)\t1\n",
      "  (1, 4037)\t1\n",
      "  (1, 154)\t2\n",
      "  (1, 2349)\t1\n",
      "  (1, 5507)\t1\n",
      "  (1, 208)\t1\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0  208 4421 6141\n",
      "  4650 5911]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0 2294 4037  154 2349\n",
      "  5507  208]]\n"
     ]
    }
   ],
   "source": [
    "X = df['predictors']#word_vectors\n",
    "y = df['binary']\n",
    "\n",
    "# Now we can create our test and training set\n",
    "trainX, testX, trainY, testY = train_test_split(X, y,\n",
    "test_size=0.33, random_state=1)\n",
    "\n",
    "# and Convert the 'predictors' column into vectors (bag of word)\n",
    "vectorizer = CountVectorizer()\n",
    "print(trainX[:2])\n",
    "\n",
    "trainX = vectorizer.fit_transform(trainX)\n",
    "testX  = vectorizer.transform(testX) # the use of vectorizer ensures that words not seen during training are ignored\n",
    "print(trainX[:2])\n",
    "print(trainX.todense())\n",
    "\n",
    "print(trainX.shape)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#print(vocab)\n",
    "\n",
    "word2idx = vectorizer.vocabulary_\n",
    "\n",
    "#print(word2idx)\n",
    "\n",
    "print(trainX.shape)\n",
    "\n",
    "print(trainX[:2])\n",
    "\n",
    "# Probably there is a direct way to deal with sparse matrices and embedding,\n",
    "# for now we will create manually the input data\n",
    "def input_from_sparse(matrix):\n",
    "    nonzeros = [val.nonzero()[1] for val in matrix]\n",
    "    return pad_sequences(nonzeros, maxlen=100, value=0.)\n",
    "# Sequence padding\n",
    "trainX = input_from_sparse(trainX)\n",
    "testX  = input_from_sparse(testX)\n",
    "\n",
    "print(trainX[:2])\n",
    "\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "# Network building\n",
    "tf.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data([None, trainX.shape[1]])\n",
    "net = tflearn.embedding(net, input_dim=len(vocab), output_dim=128)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or load an already trained NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.11814\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.11814 - acc: 0.9385 | val_loss: 0.78709 - val_acc: 0.7672 -- iter: 12478/12478\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.11814\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.11814 - acc: 0.9385 | val_loss: 0.78709 - val_acc: 0.7672 -- iter: 12478/12478\n",
      "--\n",
      "Model successfully trained and saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "# Training\n",
    "save_fn = './binary_model.tflearn'\n",
    "\n",
    "if glob.glob(save_fn + '*'):\n",
    "    print(\"*\"*80)\n",
    "    model.load(save_fn)\n",
    "    print('Model loaded from file.')\n",
    "else:\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "            batch_size=32)\n",
    "    model.save(save_fn)\n",
    "    print('Model successfully trained and saved.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should be very close to the val_acc obtained from the training\n",
      "0.765414023101\n",
      "\n",
      "Manually compare some data\n",
      "[0.06874389946460724, 0.931256115436554] [ 0.  1.]\n",
      "[4.274999355402542e-06, 0.9999957084655762] [ 0.  1.]\n",
      "[0.0001945694093592465, 0.9998055100440979] [ 0.  1.]\n",
      "[2.7417359888204373e-05, 0.9999725818634033] [ 0.  1.]\n",
      "[1.7396172324879444e-06, 0.9999982118606567] [ 0.  1.]\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.87      0.85      4675\n",
      "        1.0       0.51      0.44      0.47      1472\n",
      "\n",
      "avg / total       0.75      0.77      0.76      6147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testX)\n",
    "\n",
    "p0 = np.rint(np.array(predictions)[:, 0])\n",
    "y0 = np.rint(testY[:, 0])\n",
    "\n",
    "#print(p0[:2], y0[:2])\n",
    "print(\"This should be very close to the val_acc obtained from the training\")\n",
    "print(accuracy_score(y0, p0))\n",
    "\n",
    "\n",
    "print('\\nManually compare some data')\n",
    "for p,r in zip(predictions[:5], testY[:5]):\n",
    "    print(p,r)\n",
    "\n",
    "print('\\nClassification report')\n",
    "print(classification_report(y0, p0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12478, 100) (12478, 11)\n",
      "(6147, 100) (6147, 11)\n"
     ]
    }
   ],
   "source": [
    "X = df['predictors']\n",
    "y = df['multiple']\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, y,\n",
    "test_size=0.33, random_state=1)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "trainX = vectorizer.fit_transform(trainX)\n",
    "testX  = vectorizer.transform(testX) # the use of vectorizer ensures that words not seen during training are ignored\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2idx = vectorizer.vocabulary_\n",
    "\n",
    "# Probably there is a direct way to deal with sparse matrices and embedding,\n",
    "# for now we will create manually the input data\n",
    "def input_from_sparse(matrix):\n",
    "    nonzeros = [val.nonzero()[1] for val in matrix]\n",
    "    # Here maxlen can be tweaked\n",
    "    return pad_sequences(nonzeros, maxlen=100, value=0.)\n",
    "\n",
    "# Sequence padding\n",
    "trainX = input_from_sparse(trainX)\n",
    "testX  = input_from_sparse(testX)\n",
    "\n",
    "# Converting labels to full class vectors\n",
    "trainY = to_categorical(trainY, nb_classes=len(labels))\n",
    "testY = to_categorical(testY, nb_classes=len(labels))\n",
    "\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)\n",
    "\n",
    "# Network building\n",
    "tf.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data([None, trainX.shape[1]])\n",
    "net = tflearn.embedding(net, input_dim=len(vocab), output_dim=128)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, trainY.shape[1], activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or load an already trained NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m1.09280\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 1.09280 - acc: 0.5964 | val_loss: 1.70123 - val_acc: 0.3659 -- iter: 12478/12478\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m1.09280\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 1.09280 - acc: 0.5964 | val_loss: 1.70123 - val_acc: 0.3659 -- iter: 12478/12478\n",
      "--\n",
      "Model successfully trained and saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "save_fn = './multiple_model.tflearn'\n",
    "\n",
    "if glob.glob(save_fn + '*'):\n",
    "    print(\"*\"*80)\n",
    "    model.load(save_fn)\n",
    "    print('Model loaded from file.')\n",
    "else:\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True, batch_size=32)\n",
    "    model.save(save_fn)\n",
    "    print('Model successfully trained and saved.')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows how every single label is predicted\n",
      "0.918984870669\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.93      0.95      5607\n",
      "        1.0       0.53      0.78      0.63       540\n",
      "\n",
      "avg / total       0.94      0.92      0.93      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.965836993655\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      1.00      0.98      5937\n",
      "        1.0       0.00      0.00      0.00       210\n",
      "\n",
      "avg / total       0.93      0.97      0.95      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.926793557833\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      1.00      0.96      5697\n",
      "        1.0       0.00      0.00      0.00       450\n",
      "\n",
      "avg / total       0.86      0.93      0.89      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.999674638035\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      6145\n",
      "        1.0       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.710753212949\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.83      0.81      4565\n",
      "        1.0       0.43      0.36      0.39      1582\n",
      "\n",
      "avg / total       0.70      0.71      0.70      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.742964047503\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.89      0.84      4533\n",
      "        1.0       0.52      0.32      0.40      1614\n",
      "\n",
      "avg / total       0.72      0.74      0.72      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.998373190174\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      6137\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.891654465593\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      1.00      0.94      5481\n",
      "        1.0       0.00      0.00      0.00       666\n",
      "\n",
      "avg / total       0.80      0.89      0.84      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.848869367171\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.85      1.00      0.92      5218\n",
      "        1.0       0.00      0.00      0.00       929\n",
      "\n",
      "avg / total       0.72      0.85      0.78      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.980152920124\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      1.00      0.99      6025\n",
      "        1.0       0.00      0.00      0.00       122\n",
      "\n",
      "avg / total       0.96      0.98      0.97      6147\n",
      "\n",
      "This shows how every single label is predicted\n",
      "0.996421018383\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      6125\n",
      "        1.0       0.00      0.00      0.00        22\n",
      "\n",
      "avg / total       0.99      1.00      0.99      6147\n",
      "\n",
      "This shows the accuracy if the label is chosen based on the largest probability, it should be similar to val_acc obtained from the training\n",
      "0.371888726208\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.79      0.63       540\n",
      "          1       0.00      0.00      0.00       210\n",
      "          2       0.00      0.00      0.00       450\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.39      0.50      0.44      1582\n",
      "          5       0.50      0.35      0.41      1614\n",
      "          6       0.00      0.00      0.00        10\n",
      "          7       0.24      0.21      0.22       666\n",
      "          8       0.23      0.39      0.29       929\n",
      "          9       0.00      0.00      0.00       122\n",
      "         10       0.00      0.00      0.00        22\n",
      "\n",
      "avg / total       0.34      0.37      0.34      6147\n",
      "\n",
      "\n",
      "Manually compare some data\n",
      "[0.024708691984415054, 0.0008046197472140193, 0.0014118924736976624, 1.845431484071014e-06, 0.026443930342793465, 0.9252505898475647, 0.002064778935164213, 0.006031438242644072, 0.012950564734637737, 9.073856199393049e-05, 0.0002408049040241167] [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "[0.0012441865401342511, 0.047783516347408295, 0.07886587083339691, 8.206933125620708e-05, 0.23245610296726227, 0.05538521707057953, 1.7658327124081552e-05, 0.15298637747764587, 0.4087580442428589, 0.017554471269249916, 0.004866390954703093] [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "[0.19163550436496735, 8.001799142220989e-05, 0.00013364272308535874, 3.8641329069832864e-07, 0.0008965809247456491, 0.7990255355834961, 0.006353543605655432, 0.0007323684403672814, 0.0011128607438877225, 8.920299478631932e-06, 2.062685598502867e-05] [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "[0.005401473958045244, 0.0034995710011571646, 0.007269160356372595, 1.6236270312219858e-05, 0.5842586755752563, 0.2477143108844757, 0.00023858821077737957, 0.02068312279880047, 0.12845095992088318, 0.0009292933973483741, 0.0015386295272037387] [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "[0.7226015329360962, 9.411386417923495e-05, 0.00014593053492717445, 1.6899476804610458e-06, 6.454812682932243e-05, 0.2583877444267273, 0.017978407442569733, 0.000526118790730834, 0.00016905619122553617, 1.649633122724481e-05, 1.4400188774743583e-05] [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "log_loss:  10414.3435473\n",
      "log_loss normalized:  1.69421564133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/py3k/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testX)\n",
    "\n",
    "for i in range(testY.shape[1]):\n",
    "    p0 = np.rint(np.array(predictions)[:, i])\n",
    "    y0 = np.rint(testY[:, i])\n",
    "\n",
    "    #print(p0[:2], y0[:2])\n",
    "    print(\"This shows how every single label is predicted\")\n",
    "    print(accuracy_score(y0, p0))\n",
    "    print('\\nClassification report')\n",
    "    print(classification_report(y0, p0))\n",
    "\n",
    "\n",
    "    \n",
    "p = np.argmax(predictions, axis=1)\n",
    "y = np.argmax(testY, axis=1)\n",
    "#print(p[:2])\n",
    "#print(predictions[:2])\n",
    "print(\"This shows the accuracy if the label is chosen based on the largest probability, it should be similar to val_acc obtained from the training\")\n",
    "print(accuracy_score(y, p))\n",
    "print('\\nClassification report')\n",
    "print(classification_report(y, p))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\nManually compare some data')\n",
    "for p,r in zip(predictions[:5], testY[:5]):\n",
    "    print(p,r)\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print(\"log_loss: \", log_loss(testY, predictions, normalize=False))\n",
    "print(\"log_loss normalized: \", log_loss(testY, predictions, normalize=True))\n",
    "\n",
    "#Manual check\n",
    "#p = np.rint(predictions)\n",
    "#print(\"log_loss normalized: \", log_loss(testY, p, normalize=True))\n",
    "#result = [np.allclose(i,j) for i, j in zip(testY, p)]\n",
    "#print(np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
